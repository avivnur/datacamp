{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing a tokenizer\n",
        "\n",
        "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have `'#1'` remain a single token.\n",
        "\n",
        "`my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"`\n",
        "\n",
        "The string is available in your workspace as `my_string`, and the patterns have been pre-loaded as `pattern1`, `pattern2`, `pattern3`, and `pattern4`, respectively.\n",
        "\n",
        "Additionally, `regexp_tokenize` has been imported from `nltk.tokenize`. You can use `regexp_tokenize(string, pattern)` with `my_string` and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer."
      ],
      "metadata": {
        "id": "An4qLTCXjbXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
        "pattern1 = '(\\\\w+|\\\\?|!)'\n",
        "pattern2 = '(\\\\w+|#\\\\d|\\\\?|!)'\n",
        "pattern3 = '(#\\\\d\\\\w+\\\\?!)'\n",
        "pattern4 = '\\\\s+'"
      ],
      "metadata": {
        "id": "88RoEQi7jazc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9bSOSkfiuk4",
        "outputId": "6f04444b-788b-4b53-f25f-615e0ffeb68e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SOLDIER',\n",
              " '1',\n",
              " 'Found',\n",
              " 'them',\n",
              " '?',\n",
              " 'In',\n",
              " 'Mercea',\n",
              " '?',\n",
              " 'The',\n",
              " 'coconut',\n",
              " 's',\n",
              " 'tropical',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "regexp_tokenize(string, pattern1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokenize(string, pattern2) # BEST"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJBJ5wbAkYXt",
        "outputId": "065c0629-9f2e-4b7e-be1e-1f5cf0914916"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SOLDIER',\n",
              " '#1',\n",
              " 'Found',\n",
              " 'them',\n",
              " '?',\n",
              " 'In',\n",
              " 'Mercea',\n",
              " '?',\n",
              " 'The',\n",
              " 'coconut',\n",
              " 's',\n",
              " 'tropical',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokenize(string, pattern3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVHB_WGpkfJl",
        "outputId": "a704bc2a-cc77-4100-8100-f737700d3786"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokenize(string, pattern4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvyfHHAUknmo",
        "outputId": "daf296fa-f719-431e-818d-d178dadc66f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex with NLTK tokenization\n",
        "\n",
        "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using `nltk` and regex. The `nltk.tokenize.TweetTokenizer` class gives you some extra methods and attributes for parsing tweets.\n",
        "\n",
        "Here, you're given some example tweets to parse using both `TweetTokenizer` and `regexp_tokenize` from the `nltk.tokenize` module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!\n",
        "\n",
        "*Unlike the syntax for the regex library, with `nltk_tokenize()` you pass the pattern as the **second** argument.*"
      ],
      "metadata": {
        "id": "HcGgholvk6GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "uXtdTEXfkotb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = [\n",
        "    'This is the best #nlp exercise ive found online! #python',\n",
        "    '#NLP is super fun! <3 #learning',\n",
        "    'Thanks @datacamp :) #nlp #python']"
      ],
      "metadata": {
        "id": "IGyFsngnlnEi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a regex pattern to find hashtags: pattern1\n",
        "pattern1 = r\"#\\w+\"\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
        "print(hashtags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s87kYF3ldRP",
        "outputId": "3031e676-6496-49b8-8507-72d99c0cd5df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#nlp', '#python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a pattern that matches both mentions (@) and hashtags\n",
        "pattern2 = r\"([@#]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EymnsFzul7e8",
        "outputId": "1cb7bc03-a4ac-4e19-8ff8-8e37ccfc5cb3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@datacamp', '#nlp', '#python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR0hvdQ8mWYN",
        "outputId": "2b92f8c2-c226-4344-fce7-7ff2739f91d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-ascii tokenization\n",
        "\n",
        "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
        "\n",
        "Here, you have access to a string called `german_text`, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
        "\n",
        "The following modules have been pre-imported from `nltk.tokenize`: `regexp_tokenize` and `word_tokenize``.\n",
        "\n",
        "Unicode ranges for emoji are:\n",
        "\n",
        "`('\\U0001F300'-'\\U0001F5FF')`, `('\\U0001F600-\\U0001F64F')`, `('\\U0001F680-\\U0001F6FF')`, and `('\\u2600'-\\u26FF-\\u2700-\\u27BF')`."
      ],
      "metadata": {
        "id": "5tpl0Q6im09m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "german_text = \"Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\""
      ],
      "metadata": {
        "id": "uzsoMXxtmssu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wHGc6eynRmD",
        "outputId": "9ee8d9f5-4233-478d-f782-4e0544dfe1db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and print all words in german_text\n",
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3L8EOfjnVTW",
        "outputId": "ac125654-8d67-49b8-db60-bbcf9d478d7f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and print only capital words\n",
        "capital_words = r\"[A-ZÜ]\\w+\"\n",
        "print(regexp_tokenize(german_text, capital_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmZSJ-NpndFw",
        "outputId": "d93eda28-0754-4834-b9c3-2c039b8144ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wann', 'Pizza', 'Und', 'Über']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and print only emoji\n",
        "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
        "print(regexp_tokenize(german_text, emoji))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8Mkxofenz-8",
        "outputId": "5e4bbe91-3719-426a-8aa6-c35fb4ddb78e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['🍕', '🚕']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4IVJ4FQn80_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}