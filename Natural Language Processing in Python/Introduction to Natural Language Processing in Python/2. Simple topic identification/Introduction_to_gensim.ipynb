{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creating and querying a corpus with gensim\n",
        "It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!\n",
        "\n",
        "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus."
      ],
      "metadata": {
        "id": "CJKi0aR_jwQv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kSRP4_TCjsOq"
      },
      "outputs": [],
      "source": [
        "articles = [[]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dictionary\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(articles)\n",
        "\n",
        "# Select the id for \"computer\": computer_id\n",
        "computer_id = dictionary.token2id.get(\"computer\")\n",
        "\n",
        "# Use computer_id with the dictionary to print the word\n",
        "print(dictionary.get(computer_id))\n",
        "\n",
        "# Create a MmCorpus: corpus\n",
        "corpus = [dictionary.doc2bow(article) for article in articles]\n",
        "\n",
        "# Print the first 10 word ids with their frequency counts from the fifth document\n",
        "print(corpus[4][:10])"
      ],
      "metadata": {
        "id": "6TPkpbrikWd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJuBimIUpCRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim bag-of-words\n",
        "Now, you'll use your new `gensim` corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!\n",
        "\n",
        "You have access to the `dictionary` and `corpus` objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis.\n",
        "\n",
        "* `defaultdict` allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument `int`, we are able to ensure that any non-existent keys are automatically assigned a default value of `0`. This makes it ideal for storing the counts of words in this exercise.\n",
        "\n",
        "* `itertools.chain.from_iterable()` allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our `corpus` object (which is a list of lists).\n",
        "\n",
        "The fifth document from `corpus` is stored in the variable `doc`, which has been sorted in descending order."
      ],
      "metadata": {
        "id": "7Zkq53I9pLFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "\n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count"
      ],
      "metadata": {
        "id": "Jjd84cRXprxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "\n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count\n",
        "\n",
        "# Create a sorted list from the defaultdict: sorted_word_count\n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "for word_id, word_count in sorted_word_count[:5]:\n",
        "    print(dictionary.get(word_id), word_count)"
      ],
      "metadata": {
        "id": "WeBghD4cr8ye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}